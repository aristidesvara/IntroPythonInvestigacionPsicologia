{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python379jvsc74a57bd0fcc1d2c210046f8d6f0962280d91bf96fd89619bab60881df3e6b70c67140f94",
   "display_name": "Python 3.7.9 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "Dirk Hovy Text Analysis in Python for Social Scientist (https://github.com/dirkhovy/text_analysis_for_social_science)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## ¿Qué hay en una palabra?\n",
    "\n",
    "Supongamos que hemos recopilado una gran cantidad de autodescripciones de empresas de sus sitios web. Queremos empezar a averiguar si existen diferencias sistemáticas, cuáles son los grandes temas y cómo actúan normalmente las empresas. La pregunta es, ¿cómo hacemos todo eso? ¿Importa que las empresas compren, compren y estén comprando, o solo queremos saber si hay algún evento de compra, sin importar cuándo? ¿Nos preocupan las preposiciones (por ejemplo, dentro, fuera, sobre, sobre) o simplemente nos distraen de lo que realmente queremos saber? ¿Cómo le decimos al programa que “Facebook adquiere Whatsapp” y “Whatsapp es adquirido por Facebook” significan lo mismo, pero “Whatsapp adquiere Facebook” no (aunque usa las mismas palabras que el primer ejemplo)?\n",
    "\n",
    "Antes de sumergirnos en las aplicaciones, echemos un vistazo al tema con el que estamos trabajando: el idioma. En esta sección, veremos la terminología para describir algunos de sus elementos básicos (morfología, sintaxis y semántica) y sus equivalentes en el texto (caracteres, palabras, oraciones). Para elegir el método correcto para cualquier pregunta de investigación relacionada con el texto que tengamos, tiene sentido pensar qué es el lenguaje y qué no es, cómo está estructurado y cómo \"funciona\".\n",
    "Esta sección no reemplaza una introducción a la lingüística, pero nos da un punto de partida sólido para nuestros propósitos. Si está interesado en leer más sobre esto, hay muchos libros de texto excelentes de introducción a la lingüística y sus diversos subcampos. Uno de los más entretenidos es Fromkin, Rodman y Hyams (2018)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "El lenguaje a menudo codifica información de manera redundante, es decir, decimos lo mismo de varias maneras: a través del significado de las palabras, sus posiciones, el contexto y muchas otras señales. Las palabras mismas constan de diferentes componentes, que son el foco de diferentes disciplinas lingüísticas: su significado (semántica), su función en una oración (sintaxis) y los prefijos y terminaciones (morfología). No todas las palabras contienen toda esta información. Y cuando trabajamos con datos textuales, es posible que no estemos interesados ​​en toda esta información. De hecho, puede resultar beneficioso eliminar parte de la información que no necesitamos.\n",
    "\n",
    "Cuando trabajamos con texto, la unidad que nos interesa depende en gran medida del problema que estamos investigando. Tradicionalmente, se trataba de un informe o un artículo. Sin embargo, cuando trabajamos con redes sociales, también puede referirse al historial de publicaciones completo de un usuario, a un solo mensaje o incluso a una oración individual. A lo largo de este tutorial, nos referiremos a todas estas unidades de texto como documentos. Debe quedar claro en el contexto qué tamaño tiene un documento. Fundamentalmente, un documento siempre representa una observación en nuestros datos. Para referirnos a la colección completa de documentos / observaciones, usamos la palabra corpus (corpora plural).\n",
    "\n",
    "El conjunto de todos los términos únicos en nuestros datos se llama vocabulario (V). Cada elemento de este conjunto se denomina tipo. Cada aparición de un tipo en los datos se denomina token. Entonces, la oración \"una buena oración es una oración que tiene buenas palabras\" tiene 10 símbolos pero solo 7 tipos (a saber, \"a\", \"bueno\", \"oración\", \"es\", \"eso\", \"tiene\" y \"palabras\"). Tenga en cuenta que los tipos también pueden incluir signos de puntuación y expresiones de varias palabras."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Descriptores de palabras\n",
    "\n",
    "#### Tokens y Splitting\n",
    "\n",
    "Imagine que estamos viendo informes y queremos filtrar oraciones cortas porque no contienen nada de interés. La forma más sencilla de hacerlo es definiendo un límite para el número de palabras. Sin embargo, puede resultar sorprendentemente complicado definir qué es una palabra. ¿Cuántas palabras hay en “Se fue a Berlín” y en “Se fue a San Luis Obispo”? La definición más general utilizada en muchos idiomas es cualquier cadena de caracteres delimitada por espacios en blanco. Sin embargo, tenga en cuenta\n",
    "que el chino, por ejemplo, no usa espacios en blanco entre palabras. Desafortunadamente, no todas las palabras están rodeadas de espacios en blanco. Las palabras al principio de una línea no tienen espacios en blanco de antemano. Las palabras al final de una frase u oración pueden tener un símbolo de puntuación (comas, puntos, exclamación o signos de interrogación, etc.) directamente adjunto. Comillas y corchetes\n",
    "complica aún más las cosas.\n",
    "\n",
    "Por supuesto, podemos separar estos símbolos introduciendo espacios en blanco adicionales. Este proceso se llama tokenización (porque hacemos que cada palabra y signo de puntuación sea un token separado). Un proceso diferente, llamado división de oraciones, separa un documento en oraciones y funciona de manera similar. El problema es decidir cuándo un punto es parte de una palabra (como en títulos como Sr. o abreviaturas como abbr.) O un punto al final de una oración. Tanto la tokenización como la división de oraciones son tareas de predicción, para las cuales existen modelos confiables de aprendizaje automático. No discutiremos el funcionamiento interno de estas herramientas en detalle aquí, sino que nos basaremos en las implementaciones de Python disponibles en la biblioteca spacy.\n",
    "\n",
    "Necesitamos cargar la biblioteca y crear una instancia para el idioma con el que trabajamos (aquí, inglés), y luego podemos usarlo en cualquier cadena de entrada.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.0.6-cp37-cp37m-win_amd64.whl (11.7 MB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\renat\\anaconda3\\lib\\site-packages (from spacy) (2.25.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\renat\\anaconda3\\lib\\site-packages (from spacy) (1.19.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\renat\\anaconda3\\lib\\site-packages (from spacy) (4.55.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\renat\\anaconda3\\lib\\site-packages (from spacy) (2.11.2)\n",
      "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in c:\\users\\renat\\appdata\\roaming\\python\\python37\\site-packages (from spacy) (3.7.4.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\renat\\anaconda3\\lib\\site-packages (from spacy) (20.8)\n",
      "Requirement already satisfied: setuptools in c:\\users\\renat\\anaconda3\\lib\\site-packages (from spacy) (51.0.0.post20201207)\n",
      "Collecting blis<0.8.0,>=0.4.0\n",
      "  Downloading blis-0.7.4-cp37-cp37m-win_amd64.whl (6.5 MB)\n",
      "Collecting catalogue<2.1.0,>=2.0.3\n",
      "  Downloading catalogue-2.0.4-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\renat\\anaconda3\\lib\\site-packages (from catalogue<2.1.0,>=2.0.3->spacy) (3.4.0)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.5-cp37-cp37m-win_amd64.whl (35 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.5-cp37-cp37m-win_amd64.whl (20 kB)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\renat\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Collecting pathy>=0.3.5\n",
      "  Downloading pathy-0.5.2-py3-none-any.whl (42 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.5-cp37-cp37m-win_amd64.whl (108 kB)\n",
      "Collecting pydantic<1.8.0,>=1.7.1\n",
      "  Downloading pydantic-1.7.4-cp37-cp37m-win_amd64.whl (1.7 MB)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\renat\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\renat\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\renat\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\renat\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
      "Collecting smart-open<4.0.0,>=2.2.0\n",
      "  Downloading smart_open-3.0.0.tar.gz (113 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.4\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datadotworld 1.7.0 requires click<7.0a,>=6.0, but you have click 7.1.2 which is incompatible.\n",
      "  Downloading spacy_legacy-3.0.5-py2.py3-none-any.whl (12 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.1\n",
      "  Downloading srsly-2.4.1-cp37-cp37m-win_amd64.whl (450 kB)\n",
      "Collecting thinc<8.1.0,>=8.0.3\n",
      "  Downloading thinc-8.0.3-cp37-cp37m-win_amd64.whl (1.0 MB)\n",
      "Collecting typer<0.4.0,>=0.3.0\n",
      "  Downloading typer-0.3.2-py3-none-any.whl (21 kB)\n",
      "Collecting click<7.2.0,>=7.1.1\n",
      "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "Collecting wasabi<1.1.0,>=0.8.1\n",
      "  Downloading wasabi-0.8.2-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\renat\\anaconda3\\lib\\site-packages (from jinja2->spacy) (1.1.1)\n",
      "Building wheels for collected packages: smart-open\n",
      "  Building wheel for smart-open (setup.py): started\n",
      "  Building wheel for smart-open (setup.py): finished with status 'done'\n",
      "  Created wheel for smart-open: filename=smart_open-3.0.0-py3-none-any.whl size=107097 sha256=a3a429362446845593db4364af2ad7515ee5987b210d2007bba1d1a4c616375e\n",
      "  Stored in directory: c:\\users\\renat\\appdata\\local\\pip\\cache\\wheels\\83\\a6\\12\\bf3c1a667bde4251be5b7a3368b2d604c9af2105b5c1cb1870\n",
      "Successfully built smart-open\n",
      "Installing collected packages: murmurhash, cymem, click, catalogue, wasabi, typer, srsly, smart-open, pydantic, preshed, blis, thinc, spacy-legacy, pathy, spacy\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 6.7\n",
      "    Uninstalling click-6.7:\n",
      "      Successfully uninstalled click-6.7\n",
      "Successfully installed blis-0.7.4 catalogue-2.0.4 click-7.1.2 cymem-2.0.5 murmurhash-1.0.5 pathy-0.5.2 preshed-3.0.5 pydantic-1.7.4 smart-open-3.0.0 spacy-3.0.6 spacy-legacy-3.0.5 srsly-2.4.1 thinc-8.0.3 typer-0.3.2 wasabi-0.8.2\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting es-core-news-sm==3.0.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.0.0/es_core_news_sm-3.0.0-py3-none-any.whl (13.9 MB)\n",
      "Requirement already satisfied: spacy<3.1.0,>=3.0.0 in c:\\users\\renat\\anaconda3\\lib\\site-packages (from es-core-news-sm==3.0.0) (3.0.6)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\renat\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (1.0.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\renat\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (51.0.0.post20201207)\n",
      "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in c:\\users\\renat\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (1.7.4)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in c:\\users\\renat\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (0.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\renat\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (20.8)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\renat\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (0.7.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\renat\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (2.25.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\renat\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (0.8.2)\n",
      "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in c:\\users\\renat\\appdata\\roaming\\python\\python37\\site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (3.7.4.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\renat\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (2.0.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\renat\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (3.0.5)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.3 in c:\\users\\renat\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (8.0.3)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\renat\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (0.5.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\renat\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (2.11.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.4 in c:\\users\\renat\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (3.0.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\renat\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (4.55.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in c:\\users\\renat\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (2.0.4)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\renat\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (2.4.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\renat\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (1.19.2)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\renat\\anaconda3\\lib\\site-packages (from catalogue<2.1.0,>=2.0.3->spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (3.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\renat\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in c:\\users\\renat\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (3.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\renat\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (1.25.11)2021-06-06 19:04:38.288333: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\n",
      "2021-06-06 19:04:38.288939: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\renat\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\renat\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\renat\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (2020.12.5)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in c:\\users\\renat\\anaconda3\\lib\\site-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\renat\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (1.1.1)\n",
      "Installing collected packages: es-core-news-sm\n",
      "Successfully installed es-core-news-sm-3.0.0\n",
      "✔ Download and installation successful\n",
      "You can now load the package via spacy.load('es_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['Estuve',\n",
       "  '2',\n",
       "  'veces',\n",
       "  'en',\n",
       "  'Nueva',\n",
       "  'York',\n",
       "  'en',\n",
       "  '2011',\n",
       "  ',',\n",
       "  'pero',\n",
       "  'no',\n",
       "  'tenía',\n",
       "  'la',\n",
       "  'constitución',\n",
       "  'para',\n",
       "  'ello',\n",
       "  '.'],\n",
       " ['No', 'me', 'atrajo', '.'],\n",
       " ['Prefiero', 'Los', 'Angeles', '.']]"
      ]
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "documents = \"Estuve 2 veces en Nueva York en 2011, pero no tenía la constitución para ello. No me atrajo. Prefiero Los Angeles.\"\n",
    "\n",
    "# Aplicar la división y tokenización de oraciones a un conjunto de documentos.\n",
    "tokens = [[token.text for token in sentence] for sentence in nlp(documents).sents]\n",
    "tokens"
   ]
  },
  {
   "source": [
    "#### Lematización\n",
    "\n",
    "Supongamos que tenemos un gran corpus de artículos de la sección comercial de varios periódicos. Nos interesa saber con qué frecuencia y qué empresas se adquieren entre sí. Tenemos una lista de palabras, pero las palabras vienen en diferentes formas, según el tiempo y el aspecto, por lo que es posible que tengamos que buscar \"adquirir\", \"adquirió\", \"adquiere\" y \"adquirirá\" (ya que estamos tratando con artículos de periódicos, pueden estar refiriéndose a eventos recientes o planes futuros, y pueden citar a personas). Podríamos intentar formalizar este patrón buscando solo los finales –ir,–rió, –ere y –irá, pero eso solo funciona para verbos regulares que terminan en –ir. Si nos interesan los verbos irregulares, es posible que veamos formas como \"ir\", \"fue\", \"yendo\", o \"irá\". Y no se detiene ahí: las empresas que buscamos pueden adquirir una sola \"subsidiaria\" o varias \"subsidiarias\", una \"empresa\" o varias \"empresas\". Necesitamos una forma más basada en principios para lidiar con esta variación.\n",
    "\n",
    "Cuando buscamos una palabra en un diccionario, generalmente buscamos la **forma base** (en el ejemplo anterior, ese sería el infinitivo \"ir\"). Esta forma de base de diccionario se llama **lema**. Todas las otras formas no cambian el significado central de este lema, pero agregan más información (como aspectos temporales y de otro tipo). Muchas de estas inflexiones son requeridas por la sintaxis, es decir, el contexto y el orden de las palabras que hacen que una oración sea gramatical. Cuando trabajamos con texto y estamos más interesados ​​en el significado que en la morfología o la sintaxis, puede ser útil reemplazar cada palabra con su lema. Este paso reduce la cantidad de variación en los datos y facilita la recopilación de estadísticas significativas. En lugar de obtener un solo recuento para cada uno de \"ir\", \"fue\", \"irá\" y \"yendo\", simplemente registraríamos haber visto la forma \"ir\" cinco veces en nuestros datos. Esta reducción pierde parte de la información temporal y sintáctica, pero esa información puede ser irrelevante para nuestros propósitos. Muchas palabras se pueden lematizar deshaciendo ciertos patrones, según el tipo de palabra. Para las excepciones (por ejemplo, \"para ir\"), tenemos que tener una tabla de búsqueda.\n",
    "\n",
    "Afortunadamente, la lematización ya está incorporada en `spaCy`, por lo que podemos hacer uso de ella. Aplicando lematización a nuestras oraciones de ejemplo de arriba, obtenemos:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['Estuve',\n",
       "  '2',\n",
       "  'vez',\n",
       "  'en',\n",
       "  'Nueva',\n",
       "  'York',\n",
       "  'en',\n",
       "  '2011',\n",
       "  ',',\n",
       "  'pero',\n",
       "  'no',\n",
       "  'tener',\n",
       "  'el',\n",
       "  'constitución',\n",
       "  'para',\n",
       "  'él',\n",
       "  '.'],\n",
       " ['no', 'yo', 'atraer', '.'],\n",
       " ['preferir', 'el', 'Angeles', '.']]"
      ]
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "source": [
    "lemmas = [[token.lemma_ for token in sentence] for sentence in nlp(documents).sents]\n",
    "lemmas"
   ]
  },
  {
   "source": [
    "### Truncación (Stemming)\n",
    " \n",
    "Quizás estemos buscando textos legales para decisiones políticas y en general estemos interesados ​​en todo lo que tenga que ver con la constitución. Así que las palabras “constitución”, “constituciones”, “constitucional”, “constitucionalidad” y “constitucionalismo” son todas de interés para nosotros, a pesar de tener diferentes partes del discurso. La lematización no nos ayudará aquí, por lo que necesitamos otra forma de agrupar las palabras en clases de palabras.\n",
    "\n",
    "Una forma aún más radical de reducir la variación es la reducción. En lugar de reducir una palabra al lema, eliminamos todo menos el núcleo morfológico irreductible (la **raíz**). Por ejemplo, para una palabra como \"anticonstitucionalismo\", que se puede analizar como \"anti + constitut + ion + al + ismo\", eliminamos todo menos \"constitut\". La herramienta de truncación más famosa y utilizada se basa en el algoritmo desarrollado por Porter (1980). Para cada idioma, define una cantidad de **sufijos** (es decir, terminaciones de palabras) y el orden en el que deben eliminarse o reemplazarse. Al aplicar repetidamente estas acciones, reducimos todas las palabras a su raíz. En nuestro ejemplo, todas las palabras derivan de la raíz \"constitut–\" añadiendo diferentes terminaciones.\n",
    "\n",
    "Nuevamente, una versión del lematizador Porter ya está disponible en Python, en la biblioteca NLTK (Loper & Bird, 2002), pero tenemos que especificar el idioma."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['estuv',\n",
       "  '2',\n",
       "  'vec',\n",
       "  'en',\n",
       "  'nuev',\n",
       "  'york',\n",
       "  'en',\n",
       "  '2011',\n",
       "  ',',\n",
       "  'per',\n",
       "  'no',\n",
       "  'ten',\n",
       "  'la',\n",
       "  'constitu',\n",
       "  'par',\n",
       "  'ello',\n",
       "  '.'],\n",
       " ['no', 'me', 'atraj', '.'],\n",
       " ['prefier', 'los', 'angel', '.']]"
      ]
     },
     "metadata": {},
     "execution_count": 63
    }
   ],
   "source": [
    "from nltk import SnowballStemmer\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "#Aplicar la truncación a un conjunto de documentos\n",
    "stems = [[stemmer.stem(token) for token in sentence] for sentence in tokens]\n",
    "stems"
   ]
  },
  {
   "source": [
    "Si bien esto es extremadamente efectivo para reducir la variación en los datos, puede dificultar la interpretación de los resultados. Una forma de abordar este problema es realizar un seguimiento de todas las palabras originales que comparten una raíz y cuántas veces se produjo cada una de ellas. Entonces podemos reemplazar la raíz con la forma derivada más común (en nuestro ejemplo, esto probablemente sería \"constitución\"). Tenga en cuenta que esto puede combinar clases de palabras, digamos, sustantivos y verbos, por lo que debemos decidir si usar esto según nuestros objetivos."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### N-Gramas\n",
    "\n",
    "Mirar las palabras individualmente puede ser un buen comienzo. Sin embargo, a menudo queremos mirar también el contexto inmediato. En nuestro ejemplo, tenemos dos conceptos que abarcan dos palabras, “Nueva York” y “Los Angeles”, y no los capturamos mirando cada palabra por separado.\n",
    "\n",
    "En lugar de mirar cada palabra por separado, podemos usar una ventana deslizante de n palabras para examinar el texto. Esta ventana se llama n-grama, donde n puede tener cualquier tamaño. Las palabras individuales también se denominan unigramas, mientras que las combinaciones de dos o tres palabras se denominan bigramas y trigramas, respectivamente. Para n mayor, simplemente escribimos el número, por ejemplo, \"4-grama\".\n",
    "\n",
    "Podemos extraer n-gramas con una función de NLTK:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('Estuve', '2'),\n",
       " ('2', 'veces'),\n",
       " ('veces', 'en'),\n",
       " ('en', 'Nueva'),\n",
       " ('Nueva', 'York'),\n",
       " ('York', 'en'),\n",
       " ('en', '2011'),\n",
       " ('2011', ','),\n",
       " (',', 'pero'),\n",
       " ('pero', 'no'),\n",
       " ('no', 'tenía'),\n",
       " ('tenía', 'la'),\n",
       " ('la', 'constitución'),\n",
       " ('constitución', 'para'),\n",
       " ('para', 'ello'),\n",
       " ('ello', '.')]"
      ]
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "bigrams = [gram for gram in ngrams(tokens[0], 2)]\n",
    "bigrams"
   ]
  },
  {
   "source": [
    "Más adelante veremos cómo podemos unir expresiones de bigrama frecuentes que forman parte de la misma “palabra”."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Partes del discurso\n",
    "\n",
    "Supongamos que hemos recopilado una gran cantidad de menús de restaurantes, que incluyen tanto restaurantes de alta cocina como de comida rápida. Queremos saber cómo cada uno de ellos describe la comida. ¿Es simplemente \"a la parrilla\" o es \"jugoso\", \"fresco\" o incluso \"artesanal\"? Todos estos descriptores de alimentos son adjetivos y, al extraerlos y correlacionarlos con el tipo de restaurante que los usa, podemos aprender algo sobre la autorrepresentación de los restaurantes y sobre la correlación con el precio.\n",
    "\n",
    "En un nivel muy alto, las palabras denotan cosas, acciones y cualidades en el mundo. Estas categorías corresponden a partes del habla, por ejemplo, sustantivos, verbos y adjetivos (la mayoría de los idiomas tienen más categorías que estas tres). Se denominan conjuntamente palabras de contenido o palabras de clase abierta porque podemos agregar nuevas palabras a cada una de estas categorías (por ejemplo, nuevos sustantivos, como \"tweet\", o verbos, como \"twerking\"). Hay otras clases de palabras: determinantes, preposiciones, etc. No \"significan\" nada (es decir, no se refieren a un concepto), pero ayudan a estructurar una oración y hacerla gramatical. Por lo tanto, se denominan conjuntamente **palabras funcionales** o **palabras de clase cerrada** (es muy poco probable que a alguien se le ocurra una nueva preposición en el corto plazo). En parte porque las palabras funcionales son tan cortas y omnipresentes, a menudo se pasan por alto. Si bien tenemos una idea aproximada de cuántas veces hemos visto el sustantivo \"clase\" en las últimas oraciones, es casi imposible darse cuenta conscientemente con qué frecuencia hemos visto, digamos, \"en\".\n",
    "\n",
    "Los idiomas difieren en la forma en que estructuran las oraciones. En consecuencia, hubo poco acuerdo sobre el número exacto de estas categorías gramaticales, más allá de las tres grandes palabras de contenido (e incluso esas no siempre son seguras). La necesidad de que las herramientas de NLP funcionen en todos los idiomas generó recientemente esfuerzos para crear un pequeño conjunto de categorías que se apliquen a una amplia gama de idiomas (Petrov, Das y McDonald, 2011). Se denomina conjunto de etiquetas de parte del discurso universal (consulte https://universaldependencies.org/u/pos/). En este tutorial, usaremos este conjunto de 15 partes del discurso.\n",
    "\n",
    "Palabras de clase abierta:\n",
    "- ADJ: Adjetivos. Modifican sustantivos para especificar sus propiedades. Ejemplos: impresionante, rojo, aburrido\n",
    "- ADV: Adverbios. Modifican los verbos, pero también sirven como marcadores de preguntas. Ejemplos: tranquilamente, donde, nunca\n",
    "- INTJ: Interjecciones. Exclamaciones de algún tipo. Ejemplos: ouch, shhh, oi\n",
    "- NOUN: Sustantivos. Entidades del mundo. Ejemplos: libro, guerra, tiburón\n",
    "- PROPN: Nombres propios. Nombres de entidades, una subclase de sustantivos. Ejemplos: Rosa, Twitter, CNN\n",
    "- VERB: Verbos completos. Eventos en el mundo. Ejemplos: códigos, enviados, correctos\n",
    "\n",
    "Palabras de clase cerrada:\n",
    "- ADP: Adposiciones. Preposiciones o posposiciones, marcadores de tiempo, lugar, beneficiario, etc. Ejemplos: terminado, antes, (bajar)\n",
    "- AUX: Verbos auxiliares y modales. Se usa para cambiar de horario o modalidad. Ejemplos: ha (sido), podría (hacer), será (cambiar)\n",
    "- CCONJ: Conjunciones coordinadoras. Vincular partes de oraciones con igual importancia. Ejemplos: y, o, pero\n",
    "- DET: Determinantes. Artículos y cuantificadores. Ejemplos: a, ellos, que\n",
    "- NUM: Números. Exactamente lo que pensarías que es:::\n",
    "- PART: Partículas. Posesivos y marcadores gramaticales. Ejemplos: ’s\n",
    "- PRON: Pronombres. Sustituciones de sustantivos. Ejemplos: tú, ella, él, yo\n",
    "- SCONJ: Conjunciones subordinadas. Vincula partes de oraciones con una parte que sea más importante. Ejemplos: ya que, si, eso\n",
    "\n",
    "Otros:\n",
    "- PUNCT: Signos de puntuación. Ejemplos:!,?, -\n",
    "- SYM: Símbolos. Entidades parecidas a palabras, a menudo caracteres especiales, incluidos emojis. Ejemplos:%, $, :)\n",
    "- X: Otro. Todo lo que no se ajuste a ninguno de los anteriores. Ejemplos: pffffrt\n",
    "\n",
    "La determinación automática de las partes del discurso y la sintaxis de una oración se conoce como etiquetado y análisis de POS, dos de las primeras y más exitosas aplicaciones de NLP. Podemos usar nuevamente el etiquetador POS de `spaCy`:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['PROPN',\n",
       "  'NUM',\n",
       "  'NOUN',\n",
       "  'ADP',\n",
       "  'PROPN',\n",
       "  'PROPN',\n",
       "  'ADP',\n",
       "  'NOUN',\n",
       "  'PUNCT',\n",
       "  'CCONJ',\n",
       "  'ADV',\n",
       "  'VERB',\n",
       "  'DET',\n",
       "  'NOUN',\n",
       "  'ADP',\n",
       "  'PRON',\n",
       "  'PUNCT'],\n",
       " ['ADV', 'PRON', 'VERB', 'PUNCT'],\n",
       " ['VERB', 'DET', 'PROPN', 'PUNCT']]"
      ]
     },
     "metadata": {},
     "execution_count": 65
    }
   ],
   "source": [
    "pos = [[token.pos_ for token in sentence] for sentence in nlp(documents).sents]\n",
    "pos"
   ]
  },
  {
   "source": [
    "Debido a que el etiquetado POS fue una de las primeras aplicaciones exitosas de PNL, se han realizado muchas investigaciones al respecto. Por ahora, los etiquetadores de POS son más precisos, más consistentes y definitivamente mucho más rápidos que incluso los lingüistas mejor capacitados."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Stopwords\n",
    "\n",
    "Si queremos evaluar los temas generales en las revisiones de productos, generalmente no nos importa si una revisión se refiere al \"precio\" o \"un precio\", y podemos eliminar el determinante. Del mismo modo, si miramos la posición política de los partidos en sus manifiestos, sabemos quién escribió cada manifiesto. Por tanto, no es necesario que conservemos sus nombres cuando se mencionen a sí mismos en el documento. En ambos casos, tenemos un conjunto de palabras que aparecen con frecuencia, pero que no aportan mucho a nuestra tarea, por lo que puede ser beneficioso eliminarlas. El conjunto de estas palabras ignorables es **stopwords**.\n",
    "\n",
    "Como hemos visto anteriormente, muchas palabras de un texto pertenecen al conjunto de palabras funcionales. De hecho, las palabras más frecuentes en cualquier idioma son predominantemente palabras funcionales. Si bien todas estas son palabras muy útiles al construir una oración, en realidad no significan mucho por sí mismas, es decir, sin contexto. De hecho, para la mayoría de las aplicaciones (por ejemplo, modelos de temas; consulte Boyd-Graber, Mimno y Newman, 2014), es mejor ignorarlos por completo.\n",
    "\n",
    "Podemos excluir las stopwords en función de su parte del discurso (ver más arriba) o mediante el uso de una lista. El primero es más general, pero corre el riesgo de descartar a algunos candidatos no deseados. (Si excluimos las preposiciones, corremos el riesgo de perder el sustantivo \"redondo\" si se etiqueta incorrectamente). Este último es más específico de la tarea (por ejemplo, podemos usar una lista de nombres de partidos políticos), pero corre el riesgo de omitir las palabras que estábamos desconoce al compilar la lista. Por lo tanto, a menudo puede resultar beneficioso utilizar una combinación de ambos. En `spaCy`, podemos usar la propiedad `is_stop` de un token, que lo compara con una lista de palabras vacías comunes.\n",
    "\n",
    "Para nuestro ejemplo en ejecución, si excluimos las palabras vacías comunes y filtramos las palabras que no contienen contenido (todas las partes del discurso excepto NOUN, VERB, PROPN, ADJ y ADV):"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['Estuve', 'York', '2011', 'constitución'],\n",
       " ['atrajo'],\n",
       " ['Prefiero', 'Angeles']]"
      ]
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "source": [
    "# Seleccionar palabras de contenido a partir de POS.\n",
    "content = [[token.text for token in sentence if token.pos_ in {'NOUN', 'VERB', 'PROPN', 'ADJ', 'ADV'} \n",
    "            and not token.is_stop] for sentence in nlp(documents).sents]\n",
    "content"
   ]
  },
  {
   "source": [
    "Otra forma de reducir la variación es reemplazar los números con una ficha especial, en lugar de eliminarlos. Hay infinitos números posibles y, en muchos contextos, no nos importa la cantidad exacta que denotan (a menos que, por supuesto, estemos interesados en los precios). Veremos un ejemplo de cómo hacer esto de manera más eficiente en la Sección 3, cuando aprendamos sobre las expresiones regulares."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Entidades nombradas\n",
    "\n",
    "Digamos que estamos interesados ​​en los patrones cambiantes del turismo y queremos encontrar los destinos más populares a lo largo del tiempo en un corpus de blogs de viajes. Nos gustaría una forma de identificar los nombres de países, ciudades y puntos de referencia. Al usar el etiquetado de POS, podemos identificar fácilmente todos los nombres propios, pero eso aún no nos dice con qué tipo de entidad estamos tratando. Estas categorías semánticas de palabras se denominan entidades con nombre.\n",
    "\n",
    "Los nombres propios se refieren a entidades del mundo real, como empresas, lugares, personas o cualquier otra cosa, por ejemplo, Apple, Italia, George Oscar Bluth, LAX o Mercedes. Si bien la implementación de un reconocedor de entidades con nombre (NER) está fuera del alcance de este trabajo, generalmente funciona mediante el uso de una lista de entidades conocidas que es poco probable que cambien (por ejemplo, nombres de países o nombres). Sin embargo, los nombres son probablemente la clase de palabras más abierta e innovadora, y hay muchas entidades que no figuran en ninguna parte. Afortunadamente, a menudo podemos determinar qué tipo de entidad tenemos observando el contexto en el que ocurren. Por ejemplo, los nombres de las empresas pueden ir seguidos de \"Inc.\" o \"LLC\". La sintaxis puede darnos más pistas: si una entidad dice algo, come, duerme o está asociada con otros verbos que denotan actividades humanas, podemos etiquetarla como persona. (Por supuesto, esto se vuelve más complicado cuando tenemos un lenguaje metafórico, por ejemplo, “El parlamento dice:::”).\n",
    "\n",
    "Afortunadamente, `spaCy` proporciona un reconocedor de entidad con nombre que puede ayudarnos a identificar una amplia gama de tipos: PERSON, NORP (nacionalidad O grupo religioso o político), FAC (instalación), ORG (organización), GPE (Entidad geopolítica), LOC (ubicaciones , como mares o montañas), PRODUCT, EVENT (en deportes, política, historia, etc.), WORK_OF_ART, LAW, LANGUAGE, DATE, TIME, PERCENTAJG, MONEY, QUANTITY, ORDINAL (números ordinales como \"tercero\"), y CARDINAL (números regulares).\n",
    "\n",
    "Para nuestro ejemplo de ejecución, podemos usar el siguiente código para extraer las palabras y sus tipos:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[('Estuve 2', 'MISC'), ('Nueva York', 'LOC')], [], [('Los Angeles', 'LOC')]]"
      ]
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "source": [
    "entities = [[(entity.text , entity.label_) for entity in nlp(sentence.text).ents] for sentence in nlp(documents).sents]\n",
    "entities"
   ]
  },
  {
   "source": [
    ">Nota: Aquí este procedimiento puede fallar debido a limitaciones idiomáticas. La mayoría de los modelos y funciones de NLP están diseñadas para el inglés, y cuando se cambia de idioma el desempeño de estas puede reducirse. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Sintaxis\n",
    "\n",
    "Digamos que estamos interesados ​​en adquisiciones firmes. Imagine que queremos filtrar todas las transacciones comerciales (quién compró a quién) de un gran corpus de texto de Newswire. Definimos una lista de verbos que expresan adquisiciones y queremos encontrar todos los sustantivos asociados con estos verbos. Es decir, quién adquiere (los sujetos) o quién adquiere (los objetos). Queremos terminar con triples NOUN-VERB-NOUN que nos den los nombres de las firmas y otras entidades involucradas en las adquisiciones, así como el verbo.\n",
    "\n",
    "Ya hemos visto que las palabras pueden tener diferentes formas gramaticales, es decir, partes del discurso (sustantivos, verbos, etc.). Sin embargo, una oración puede tener muchas palabras con el mismo POS, y lo que significa una palabra específica en una oración depende en parte de su función gramatical. La estructura de la oración, o sintaxis, es un campo de estudio esencial en lingüística y explica cómo las palabras de una oración se unen.\n",
    "\n",
    "Una de las claves más importantes de la función de una palabra en inglés es su posición en la oración. En inglés, la función gramatical está determinada principalmente por el orden de las palabras, mientras que otros idiomas usan marcadores en las palabras. Cambiar el orden de las palabras cambia la función gramatical de la palabra. Considere la diferencia entre \"El nadador come pescado\" y \"El pescado come al nadador\". El sujeto (la entidad que come) y el objeto (la entidad que se come) ahora se cambian. Ya hemos visto verbos, sujetos y objetos como ejemplos de funciones sintácticas, pero hay muchas otras posibles relaciones de dependencia que pueden darse entre palabras. La idea en la gramática de dependencia es que la oración “cuelga” del verbo principal como un móvil. Los vínculos entre palabras describen cómo están conectadas las palabras.\n",
    "\n",
    "Naturalmente, estas conexiones pueden variar bastante entre idiomas, pero de manera similar a las etiquetas POS Universal, se han realizado esfuerzos para unificar y limitar el conjunto de posibilidades (McDonald et al., 2013; Nivre et al., 2015, 2016, entre otros. alia). Aquí hay una lista de los que se usaron en el proyecto de dependencias universales (nuevamente, consulte https://universaldependencies.org):\n",
    "\n",
    "- acl, modificador clausal de un sustantivo (cláusula adjetiva)\n",
    "- advcl, modificador de cláusula adverbial\n",
    "- advmod, modificador adverbial\n",
    "- amod, modificador de adjetivo\n",
    "- appos, modificador de aposición\n",
    "- aux, verbo auxiliar\n",
    "- case, marcador de estuches\n",
    "- cc, conjunción coordinadora\n",
    "- ccomp, complemento clausal\n",
    "- clf, clasificador\n",
    "- compound, compuesto\n",
    "- conj, conjunción\n",
    "- cop, cópula\n",
    "- csubj, sujeto clausal\n",
    "- dep, dependencia no especificada\n",
    "- det, determinante\n",
    "- discourse, elemento del discurso\n",
    "- dislocated, dislocados\n",
    "- dobj, objeto directo\n",
    "- expl, improperio\n",
    "- fixed, fija de varias palabras\n",
    "- flat, plana de varias palabras\n",
    "- goeswith, va con\n",
    "- iobj, objeto indirecto\n",
    "- list, lista\n",
    "- mark, marcador\n",
    "- nmod, modificador nominal\n",
    "- nsubj, sujeto nominal\n",
    "- nummod, modificador numérico\n",
    "- obj, objeto\n",
    "- obl, nominal oblicuo\n",
    "- orphan, huérfano\n",
    "- parataxis, parataxis\n",
    "- pobj, objeto preposicional\n",
    "- puntc, puntuacion\n",
    "- reparandum, disfluencia anulada\n",
    "- root, la raíz de la oración, generalmente un verbo\n",
    "- vocative, vocativo\n",
    "- xcomp, complemento clausal abierto\n",
    "\n",
    "Es posible que muchos de estos nunca o rara vez ocurran en el lenguaje que estudiamos, y para muchas aplicaciones, es posible que solo queramos ver un pequeño puñado de relaciones, por ejemplo, los argumentos centrales, nsubj, obj, iobj, pobj o root. Podemos ver un ejemplo de un análisis de dependencia en la Figura 1. ¿Por qué es relevante? Porque saber dónde se encuentra una palabra en la oración y qué otras palabras están relacionadas con ella (y en qué función), nos ayuda a darle sentido a una oración. En Python, podemos usar el analizador de la biblioteca spaCy para obtener:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[('Estuve', 'Estuve', 'ROOT'),\n",
       "  ('2', 'Estuve', 'nummod'),\n",
       "  ('veces', 'Estuve', 'nmod'),\n",
       "  ('en', 'Nueva', 'case'),\n",
       "  ('Nueva', 'Estuve', 'nmod'),\n",
       "  ('York', 'Nueva', 'flat'),\n",
       "  ('en', '2011', 'case'),\n",
       "  ('2011', 'Estuve', 'nmod'),\n",
       "  (',', 'tenía', 'punct'),\n",
       "  ('pero', 'tenía', 'cc'),\n",
       "  ('no', 'tenía', 'advmod'),\n",
       "  ('tenía', 'Estuve', 'conj'),\n",
       "  ('la', 'constitución', 'det'),\n",
       "  ('constitución', 'tenía', 'obj'),\n",
       "  ('para', 'ello', 'case'),\n",
       "  ('ello', 'tenía', 'obl'),\n",
       "  ('.', 'Estuve', 'punct')],\n",
       " [('No', 'atrajo', 'advmod'),\n",
       "  ('me', 'atrajo', 'obj'),\n",
       "  ('atrajo', 'atrajo', 'ROOT'),\n",
       "  ('.', 'atrajo', 'punct')],\n",
       " [('Prefiero', 'Prefiero', 'ROOT'),\n",
       "  ('Los', 'Angeles', 'det'),\n",
       "  ('Angeles', 'Prefiero', 'flat'),\n",
       "  ('.', 'Prefiero', 'punct')]]"
      ]
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "source": [
    "[[(c.text , c.head.text , c.dep_) for c in nlp(sentence.text)] for sentence in nlp(documents).sents]"
   ]
  },
  {
   "source": [
    "Podemos ver cómo el verbo “estado” es el nodo raíz del que cuelga todo lo demás. Para nuestro ejemplo con las adquisiciones de firmas, querríamos extraer cualquier cosa etiquetada con nsubj, dobj o pobj.\n",
    "\n",
    "Si estás trabajando con otros idiomas además del inglés, puedes, en muchos casos, aún lematizar los datos. Sin embargo, es posible que tenga menos o ninguna opción para etiquetado POS, parsing y NER. Sin embargo, para las aplicaciones más avanzadas, un preprocesamiento adecuado suele ser más importante que poder realizar todo tipo de análisis lingüísticos. Sin embargo, si su análisis se basa en estas herramientas, eche un vistazo a la literatura de NLP sobre el procesamiento del lenguaje de bajos recursos, un subcampo afortunadamente creciente en el NLP.\n",
    "\n",
    "Ahora que hemos visto cuáles son los componentes básicos del análisis lingüístico, podemos empezar a buscar la extracción de información del texto. Veremos cómo buscar patrones flexibles en los datos para identificar cosas como direcciones de correo electrónico, nombres de empresas o cantidades numéricas. Además, veremos cómo encontrar las palabras más significativas e importantes en un corpus. (dado que la frecuencia pura no es un indicador perfecto). Y veremos cómo captar que algunas palabras ocurren con mucha más frecuencia juntas que solas, y cómo definir esta noción."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Expresiones Regulares\n",
    "\n",
    "Supongamos que está trabajando con una gran muestra de registros de empleo de empleados italianos, con información detallada sobre cuándo trabajaron en qué empresa y una descripción de lo que hicieron allí. Sin embargo, los nombres de las empresas figuran en el texto, en lugar de en una entrada separada. Desea extraer los nombres para obtener una descripción general aproximada de cuántas empresas diferentes hay y agrupar los registros en consecuencia. Desafortunadamente, la herramienta NER para italiano no funciona tan bien como le gustaría. Sin embargo, sabe que los nombres de las empresas italianas a menudo terminan en varias abreviaturas (similar a \"Co.\",\n",
    "\"Ltd.\" o \"Inc.\"). Solo hay dos que desea considerar por ahora (\"s.p.a.\" y \"s.r.l.\"), pero vienen en muchas variaciones de ortografía diferentes (por ejemplo, SPA, spa, S.P.A.). ¿Cómo puede identificar estos marcadores y extraer los nombres de las empresas justo antes de ellos?\n",
    "\n",
    "En el mismo proyecto, ha recopilado datos de encuestas de una gran muestra de empleados. Algunos de ellos han dejado su dirección de correo electrónico como parte de la respuesta y le gustaría detectar y extraer las direcciones de correo electrónico. Puede buscar cualquier cosa que venga antes o después del signo @, pero ¿cómo se asegura de que no sea un identificador de Twitter? ¿Y cómo se asegura de que sea una dirección de correo electrónico válida?\n",
    "\n",
    "La variación de cadenas es una aplicación de libro de texto de expresiones regulares (o RegEx). Son patrones flexibles que le permiten especificar lo que está buscando en general y cómo puede variar. Para hacerlo, simplemente escriba el patrón que desea hacer coincidir. La Tabla 1 muestra ejemplos de patrones simples y sus coincidencias.\n",
    "\n",
    "En Python, podemos especificar expresiones regulares y luego aplicarlas al texto con `search()` o `match()`. El primero verifica si un patrón está contenido en la entrada, el segundo verifica si el patrón coincide completamente con la entrada:\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<re.Match object; span=(1, 3), match='at'>\nNone\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "pattern = re.compile(\"at\")\n",
    "print(re.search(pattern , 'later'))\n",
    "print(re.match(pattern , 'later')) "
   ]
  },
  {
   "source": [
    "A veces, sabemos que un personaje específico estará en nuestro patrón, pero no cuántas veces. Podemos usar cuantificadores especiales para señalar que el carácter justo antes de ellos ocurre cualquier número de veces, incluyendo cero (*), exactamente una o cero veces (?), O una o más veces (+). Vea los ejemplos en la Tabla 2."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Point-wise Mutual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5.2.3 TF-IDF Count\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(analyzer='word', min_df =0.001, max_df=0.75, stop_words='spanish',sublinear_tf=True)\n",
    "X = tfidf_vectorizer.fit_transform(documents)"
   ]
  }
 ]
}