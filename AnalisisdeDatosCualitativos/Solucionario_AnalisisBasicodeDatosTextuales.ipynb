{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "fcc1d2c210046f8d6f0962280d91bf96fd89619bab60881df3e6b70c67140f94"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Ejercicio 1: Análisis preliminar de entrevistas II\n",
    "\n",
    "Continuamos con nuestro trabajo como asistente de investigación un grupo enfocado en la toma de decisiones vocacionales. Ahora se te ha pedido que hagas un análisis individual de cada entrevistado. \n",
    "\n",
    "Nuevamente cargamos la transcripción de entrevistas, provista en formato PDF:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: PyPDF2 in c:\\users\\renat\\anaconda3\\lib\\site-packages (1.26.0)\n"
     ]
    }
   ],
   "source": [
    "# Instalar el paquete PyPDF2\n",
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "92\n1\n \n \n1.\n \nEntrevista 1\n \n \nEstudiante 1 de género masculino, universidad pública, \npsicología\n, que proviene de \nliceo\n \ncientífico \n\n \nhumanista A (entrevistado el 6 de enero de 2014).\n \n \n \nIniciando mi nombre es \nentrevistado uno\n, tengo 20 años, estudio \npsicología\n \nen la \nuniversidad pública\n, estoy, pasé a tercer año de la carrera y egresé, la enseñanza media la \ncursé en el liceo\n \ncientífico \n\n \nhumanista A \nde la comuna de P. A.\n \n \nEl Liceo, ¿es municipal?\n \n \nEs municipal, si, uno de los otrora emblemáticos de \ncomuna de P. A. \nque ahora está de mal \nen peor, con mucho estudiante en riesgo social, muy vulnerable, con matrícula reducida \npara\n \nlo que es su infraestructura. No, es como quizás da cuenta la situación de la educación \nmunicipal y de la educación pública a nivel medio en Chile, que están siendo vejados en \nfunción de lo privado, eso.\n \n \nHa venido como disminuyéndose. Mira, la pregunta q\nue guía la siguiente entrevista es \nmás menos ¿cómo fue tu proceso de decisión vocacional tanto en tu etapa escolar, \nconsiderando elementos de tu historia de vida hasta más menos como has llegado \nahora, cómo llegaste en el fondo a tomar esta decisión en el \ncontexto de oportunidades \nque se te dio?\n \n \n\nahí. Bueno, cuando yo era chico, tenía alrededor de diez años pertenecí a una congregación \nreligiosa que entre otras cosas ins\ntaba a los jóvenes a hacer una especie de misión que se \nllama, en la cual tu cumples diecinueve años y tienes que servir por dos años a tu religión \nhaciendo proselitismo religioso en otro país por lo general. Entonces mis padres siempre \nme inculcaron que y\no tenía que hacer eso y posterior a ello yo podía estudiar no sé, algún \nbachillerato, algo así que ellos me iban a poder financiar la carrera. Bueno, después con los \nvaivenes de la vida yo salí afortunadamente de esa congregación religiosa, puedo decir que\n \nvi la luz al salir.\n \n \n¿Qué edad tenías ahí más menos?\n \n \nDiecisiete años cuando sucedió eso pero antes de eso igual mucho, muchas situaciones, \ncambios de empleo de mi viejo que le bajaron el sueldo, que a mi vieja le subieron el \nsueldo, muchas situaciones ca\nmbiaron, entonces ahí tu por medio de tus viejos, no tanto \nmis viejos quizás, sino que más que nada sentía la presión de que tenía que hacerlo porque \n\n \nque \n\n\nporque sino ingresas a la universidad, pucha, vas a ser un pollo, \ncachai\n.\n \n \n¿Qué significa ser un pollo?\n \n \n\n"
     ]
    }
   ],
   "source": [
    "# Importar paquetes a ser utilizados\n",
    "import matplotlib.pyplot as plt\n",
    "import PyPDF2\n",
    "import nltk\n",
    "\n",
    "# crea un objeto de archivo pdf\n",
    "pdfFileObj = open(\"Anexo_Entrevistas.pdf\", \"rb\")\n",
    "\n",
    "# crea un objeto de lectura de pdf\n",
    "pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "\n",
    "# muestra el número de páginas en un archivo pdf\n",
    "print(pdfReader.numPages)\n",
    "\n",
    "# crea un objeto de página\n",
    "pageObj = pdfReader.getPage(2)\n",
    "\n",
    "# extrae text de una página\n",
    "print(pageObj.extractText())"
   ]
  },
  {
   "source": [
    "1. Selecciona las entrevistas realizadas a los participantes 2 y 5, divide cada entrevista en oraciones y extrae los tokens de cada oración."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escribe tu respuesta aquí\n",
    "pdfText = []\n",
    "for page in range(19, 35):\n",
    "    pageObj = pdfReader.getPage(page)\n",
    "    pdfText.append(pageObj.extractText())\n",
    "raw_2 = \" \".join(pdfText)\n",
    "\n",
    "pdfText = []\n",
    "for page in range(67, 77):\n",
    "    pageObj = pdfReader.getPage(page)\n",
    "    pdfText.append(pageObj.extractText())\n",
    "raw_5 = \" \".join(pdfText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "tokens_2 = [\n",
    "    [token.text for token in sentence] for sentence in nlp(raw_2).sents\n",
    "]\n",
    "tokens_5 = [\n",
    "    [token.text for token in sentence] for sentence in nlp(raw_5).sents\n",
    "]"
   ]
  },
  {
   "source": [
    "2. Aplica lematización a las oraciones de cada entrevista."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escribe tu respuesta aquí\n",
    "lemmas_2 = [\n",
    "    \" \".join([token.lemma_ for token in sentence])\n",
    "    for sentence in nlp(raw_2).sents\n",
    "]\n",
    "lemmas_5 = [\n",
    "    \" \".join([token.lemma_ for token in sentence])\n",
    "    for sentence in nlp(raw_5).sents\n",
    "]"
   ]
  },
  {
   "source": [
    "3. Extrae los stopwords de cada entrevista.\n",
    "(Sugerencia: Utiliza la función stopwords presente en NLTK) "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escribe tu respuesta aquí\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_ = set(stopwords.words(\"spanish\"))"
   ]
  },
  {
   "source": [
    "4. Para cada entrevista, realiza un conteo de tipo TF-IDF y preséntalo en un gráfico. ¿Encuentras alguna diferencia entre las entrevistas?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escribe tu respuesta aquí\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer_2 = TfidfVectorizer(\n",
    "    analyzer=\"word\",\n",
    "    min_df=0.001,\n",
    "    max_df=0.75,\n",
    "    stop_words=stopwords_,\n",
    "    sublinear_tf=True,\n",
    ")\n",
    "X_2 = tfidf_vectorizer_2.fit_transform(lemmas_2)\n",
    "\n",
    "tfidf_vectorizer_5 = TfidfVectorizer(\n",
    "    analyzer=\"word\",\n",
    "    min_df=0.001,\n",
    "    max_df=0.75,\n",
    "    stop_words=stopwords_,\n",
    "    sublinear_tf=True,\n",
    ")\n",
    "X_5 = tfidf_vectorizer_5.fit_transform(lemmas_5)\n",
    "\n",
    "idf_2 = tfidf_vectorizer_2.idf_\n",
    "tfidf_2 = X_2.sum(axis=0).A1\n",
    "\n",
    "idf_5 = tfidf_vectorizer_5.idf_\n",
    "tfidf_5 = X_5.sum(axis=0).A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12, 7.5))\n",
    "sns.scatterplot(\n",
    "    x=tfidf_2, y=idf_2, size=tfidf_2 / idf_2, legend=False, sizes=(1, 200)\n",
    ")\n",
    "plt.xlabel(\"TF-IDF\", size=14)\n",
    "plt.ylabel(\"IDF\", size=14)\n",
    "names = tfidf_vectorizer_2.get_feature_names()\n",
    "top = (tfidf_2 / idf_2).argsort()[-5:]\n",
    "for i in top:\n",
    "    plt.text(\n",
    "        tfidf_2[i], idf_2[i], str(names[i]), size=12, va=\"bottom\", ha=\"left\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 7.5))\n",
    "sns.scatterplot(\n",
    "    x=tfidf_5, y=idf_5, size=tfidf_5 / idf_5, legend=False, sizes=(1, 200)\n",
    ")\n",
    "plt.xlabel(\"TF-IDF\", size=14)\n",
    "plt.ylabel(\"IDF\", size=14)\n",
    "names = tfidf_vectorizer_5.get_feature_names()\n",
    "top = (tfidf_5 / idf_5).argsort()[-5:]\n",
    "for i in top:\n",
    "    plt.text(\n",
    "        tfidf_5[i], idf_5[i], str(names[i]), size=12, va=\"bottom\", ha=\"left\"\n",
    "    )"
   ]
  },
  {
   "source": [
    "## Solución alternativa"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_2 = [\n",
    "    \" \".join(\n",
    "        [\n",
    "            token.text\n",
    "            for token in sentence\n",
    "            if token.pos_ in {\"NOUN\", \"VERB\", \"PROPN\", \"ADJ\", \"ADV\"}\n",
    "            and not token.is_stop\n",
    "        ]\n",
    "    )\n",
    "    for sentence in nlp(raw_2).sents\n",
    "]\n",
    "\n",
    "content_5 = [\n",
    "    \" \".join(\n",
    "        [\n",
    "            token.text\n",
    "            for token in sentence\n",
    "            if token.pos_ in {\"NOUN\", \"VERB\", \"PROPN\", \"ADJ\", \"ADV\"}\n",
    "            and not token.is_stop\n",
    "        ]\n",
    "    )\n",
    "    for sentence in nlp(raw_5).sents\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer_2 = TfidfVectorizer(\n",
    "    analyzer=\"word\",\n",
    "    min_df=0.001,\n",
    "    max_df=0.75,\n",
    "    stop_words=stopwords_,\n",
    "    sublinear_tf=True,\n",
    ")\n",
    "X_2 = tfidf_vectorizer_2.fit_transform(content_2)\n",
    "\n",
    "tfidf_vectorizer_5 = TfidfVectorizer(\n",
    "    analyzer=\"word\",\n",
    "    min_df=0.001,\n",
    "    max_df=0.75,\n",
    "    stop_words=stopwords_,\n",
    "    sublinear_tf=True,\n",
    ")\n",
    "X_5 = tfidf_vectorizer_5.fit_transform(content_5)\n",
    "\n",
    "idf_2 = tfidf_vectorizer_2.idf_\n",
    "tfidf_2 = X_2.sum(axis=0).A1\n",
    "\n",
    "idf_5 = tfidf_vectorizer_5.idf_\n",
    "tfidf_5 = X_5.sum(axis=0).A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12, 7.5))\n",
    "sns.scatterplot(\n",
    "    x=tfidf_2, y=idf_2, size=tfidf_2 / idf_2, legend=False, sizes=(1, 200)\n",
    ")\n",
    "plt.xlabel(\"TF-IDF\", size=14)\n",
    "plt.ylabel(\"IDF\", size=14)\n",
    "names = tfidf_vectorizer_2.get_feature_names()\n",
    "top = (tfidf_2 / idf_2).argsort()[-5:]\n",
    "for i in top:\n",
    "    plt.text(\n",
    "        tfidf_2[i], idf_2[i], str(names[i]), size=12, va=\"bottom\", ha=\"left\"\n",
    "    )\n",
    "    print(names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 7.5))\n",
    "sns.scatterplot(\n",
    "    x=tfidf_5, y=idf_5, size=tfidf_5 / idf_5, legend=False, sizes=(1, 200)\n",
    ")\n",
    "plt.xlabel(\"TF-IDF\", size=14)\n",
    "plt.ylabel(\"IDF\", size=14)\n",
    "names = tfidf_vectorizer_5.get_feature_names()\n",
    "top = (tfidf_5 / idf_5).argsort()[-5:]\n",
    "for i in top:\n",
    "    plt.text(\n",
    "        tfidf_5[i], idf_5[i], str(names[i]), size=12, va=\"bottom\", ha=\"left\"\n",
    "    )\n",
    "    print(names[i])"
   ]
  },
  {
   "source": [
    "# Ejercicio 2: Análisis preliminar de hasthags en Twitter II\n",
    "\n",
    "Continuamos en nuestro trabajo como consultores de Community Management. \n",
    "\n",
    "Dada la coyuntura, se te pide que profundices en tu análisis de los hasthags: `#CambioViciadoPorElLapiz` y `#CambioViciadoPorKeiko`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa las librerías a utilizar\n",
    "import tweepy\n",
    "import re\n",
    "from unicodedata import normalize\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# Variables con las credenciales de acceso al API de Twitter\n",
    "consumerKey = \"TYPE HERE\"\n",
    "consumerSecret = \"TYPE HERE\"\n",
    "accessToken = \"TYPE HERE\"\n",
    "accessTokenSecret = \"TYPE HERE\"\n",
    "\n",
    "# Acceso al API de Twitter\n",
    "auth = tweepy.OAuthHandler(consumerKey, consumerSecret)\n",
    "auth.set_access_token(accessToken, accessTokenSecret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "source": [
    "1. Extrae 2500 tweets que utilicen los hashtags mencionados y colócalos en dos listas separadas. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escribe tu respuesta aquí\n",
    "keyword_A = \"#CambioViciadoPorElLapiz\"  # keyword or hashtag to search\n",
    "keyword_B = \"#CambioViciadoPorKeiko\"\n",
    "noOfTweet = 2500  # how many tweets to analyze\n",
    "\n",
    "tweets_A = tweepy.Cursor(api.search, q=keyword_A).items(noOfTweet)\n",
    "tweets_B = tweepy.Cursor(api.search, q=keyword_B).items(noOfTweet)\n",
    "\n",
    "tweet_list_A = []\n",
    "tweet_list_B = []\n",
    "for tweet in tweets_A:\n",
    "    tweet_list_A.append(tweet.text)\n",
    "for tweet in tweets_B:\n",
    "    tweet_list_B.append(tweet.text)"
   ]
  },
  {
   "source": [
    "2. Crea dos Series de Pandas a partir de estas listas. Retira los duplicados y quita los caracteres indeseados utilizando expresiones regulares. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escribe tu respuesta aquí\n",
    "import pandas as pd\n",
    "\n",
    "ser_A = pd.Series(data=tweet_list_A)\n",
    "ser_A.drop_duplicates(keep=False, inplace=True)\n",
    "\n",
    "ser_B = pd.Series(data=tweet_list_B)\n",
    "ser_B.drop_duplicates(keep=False, inplace=True)\n",
    "\n",
    "# Puedes usar estas funciones para remover los caracteres indeseados.\n",
    "remove_rt = lambda x: re.sub(\"RT @\\w+: \", \" \", x)\n",
    "remove_tilde = lambda x: re.sub(\n",
    "    r\"([^n\\u0300-\\u036f]|n(?!\\u0303(?![\\u0300-\\u036f])))[\\u0300-\\u036f]+\",\n",
    "    r\"\\1\",\n",
    "    normalize(\"NFD\", x),\n",
    "    0,\n",
    "    re.I,\n",
    ")\n",
    "rt = lambda x: re.sub(\n",
    "    \"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", x\n",
    ")\n",
    "\n",
    "new_serA = ser_A.map(remove_rt).map(remove_tilde).map(rt)\n",
    "new_serA = new_serA.str.lower()\n",
    "\n",
    "new_serB = ser_B.map(remove_rt).map(remove_tilde).map(rt)\n",
    "new_serB = new_serB.str.lower()"
   ]
  },
  {
   "source": [
    "3. Encuentra colocaciones en las dos Series de Pandas utilizando la liberaría NLTK."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escribe tu respuesta aquí\n",
    "from nltk.collocations import BigramCollocationFinder, BigramAssocMeasures\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_ = set(stopwords.words(\"spanish\"))\n",
    "stopwords_.update([\"cambioviciadoporkeiko\", \"cambioviciadoporellapiz\"])\n",
    "\n",
    "words_A = [\n",
    "    word.lower()\n",
    "    for document in new_serA\n",
    "    for word in document.split()\n",
    "    if len(word) > 2 and word not in stopwords_\n",
    "]\n",
    "words_B = [\n",
    "    word.lower()\n",
    "    for document in new_serB\n",
    "    for word in document.split()\n",
    "    if len(word) > 2 and word not in stopwords_\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finder = BigramCollocationFinder.from_words(words_A)\n",
    "bgm = BigramAssocMeasures()\n",
    "collocations = {\n",
    "    bigram: pmi for bigram, pmi in finder.score_ngrams(bgm.mi_like)\n",
    "}\n",
    "collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finder = BigramCollocationFinder.from_words(words_B)\n",
    "bgm = BigramAssocMeasures()\n",
    "collocations = {\n",
    "    bigram: pmi for bigram, pmi in finder.score_ngrams(bgm.mi_like)\n",
    "}\n",
    "collocations"
   ]
  },
  {
   "source": [
    "4.  Para cada Serie, realiza un conteo de tipo TF-IDF y preséntalo en un gráfico. ¿Encuentras alguna diferencia entre los hashtags?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escribe tu respuesta aquí\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "tfidf_vectorizer_2 = TfidfVectorizer(\n",
    "    analyzer=\"word\",\n",
    "    min_df=0.001,\n",
    "    max_df=0.75,\n",
    "    stop_words=stopwords_,\n",
    "    sublinear_tf=True,\n",
    ")\n",
    "X_2 = tfidf_vectorizer_2.fit_transform(new_serA)\n",
    "\n",
    "tfidf_vectorizer_5 = TfidfVectorizer(\n",
    "    analyzer=\"word\",\n",
    "    min_df=0.001,\n",
    "    max_df=0.75,\n",
    "    stop_words=stopwords_,\n",
    "    sublinear_tf=True,\n",
    ")\n",
    "X_5 = tfidf_vectorizer_5.fit_transform(new_serB)\n",
    "\n",
    "idf_2 = tfidf_vectorizer_2.idf_\n",
    "tfidf_2 = X_2.sum(axis=0).A1\n",
    "\n",
    "idf_5 = tfidf_vectorizer_5.idf_\n",
    "tfidf_5 = X_5.sum(axis=0).A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12, 7.5))\n",
    "sns.scatterplot(\n",
    "    x=tfidf_2, y=idf_2, size=tfidf_2 / idf_2, legend=False, sizes=(1, 200)\n",
    ")\n",
    "plt.xlabel(\"TF-IDF\", size=14)\n",
    "plt.ylabel(\"IDF\", size=14)\n",
    "names = tfidf_vectorizer_2.get_feature_names()\n",
    "top = (tfidf_2 / idf_2).argsort()[-5:]\n",
    "for i in top:\n",
    "    plt.text(\n",
    "        tfidf_2[i], idf_2[i], str(names[i]), size=12, va=\"bottom\", ha=\"left\"\n",
    "    )\n",
    "    print(names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 7.5))\n",
    "sns.scatterplot(\n",
    "    x=tfidf_5, y=idf_5, size=tfidf_5 / idf_5, legend=False, sizes=(1, 200)\n",
    ")\n",
    "plt.xlabel(\"TF-IDF\", size=14)\n",
    "plt.ylabel(\"IDF\", size=14)\n",
    "names = tfidf_vectorizer_5.get_feature_names()\n",
    "top = (tfidf_5 / idf_5).argsort()[-5:]\n",
    "for i in top:\n",
    "    plt.text(\n",
    "        tfidf_5[i], idf_5[i], str(names[i]), size=12, va=\"bottom\", ha=\"left\"\n",
    "    )\n",
    "    print(names[i])"
   ]
  }
 ]
}